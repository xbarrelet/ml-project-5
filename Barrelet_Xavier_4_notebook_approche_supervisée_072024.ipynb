{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as gsp\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from mlflow.models import infer_signature\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from pandas import DataFrame\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import *\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_QUESTIONS_FILE = 'cached_questions_2500.json'\n",
    "RESULTS_PATH = 'supervised_results'\n",
    "MODELS_PATH = 'models/supervised'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having multiprocessing issues between BERT and the GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "mlflow.set_experiment(\"Supervised Learning Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_questions():\n",
    "    \"\"\"Load questions from the cache file.\"\"\"\n",
    "    with open(CACHED_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_saved_model():\n",
    "    \"\"\"Removes the content of the saved model.\"\"\"\n",
    "    shutil.rmtree(MODELS_PATH, ignore_errors=True)\n",
    "    os.makedirs(MODELS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_text(question: dict):\n",
    "    \"\"\"Create a new 'text' field for each question containing the cleaned, tokenized and lemmatized title + body.\"\"\"\n",
    "    title = question['title']\n",
    "    body = question['body']\n",
    "    text = f\"{title} {body}\"\n",
    "    \n",
    "    for filter in [gsp.strip_tags,\n",
    "                   gsp.strip_punctuation,\n",
    "                   gsp.strip_multiple_whitespaces,\n",
    "                   gsp.strip_numeric,\n",
    "                   gsp.remove_stopwords,\n",
    "                   gsp.strip_short,\n",
    "                   gsp.lower_to_unicode]:\n",
    "        text = filter(text)\n",
    "        \n",
    "    tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # words_stemmed = (stemmer.stem(w) for w in words_without_short_words)\n",
    "    words_lemmatized = [lemmatizer.lemmatize(w) for w in tokenized_text]\n",
    "    question['text'] = \" \".join(words_lemmatized)\n",
    "\n",
    "    # bigrams = nltk.bigrams(tokenized_text)\n",
    "    # question['bigrams'] = [' '.join(bigram) for bigram in bigrams]\n",
    "\n",
    "    # trigrams = nltk.trigrams(tokenized_text)\n",
    "    # question['trigrams'] = [' '.join(trigram) for trigram in trigrams]\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_inp_fct(sentences, bert_tokenizer, max_length):\n",
    "    \"\"\"Returns BERT variables for its prediction.\"\"\"\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    bert_inp_tot = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens=True,\n",
    "                                              max_length=max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "        \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0],\n",
    "                             bert_inp['token_type_ids'][0],\n",
    "                             bert_inp['attention_mask'][0]))\n",
    "        \n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_using_BERT(model, model_type, sentences, max_length, b_size):\n",
    "    \"\"\"Transform the text of the question's body and title into BERT embeddings.\"\"\"\n",
    "    # We don't want to use the cleaned text field with BERT, only title + \" \" + body\n",
    "    sentences = [f\"{sentence[1]} {sentence[0]}\" for sentence in sentences.iterrows()]\n",
    "    \n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    \n",
    "    time1 = time.time()\n",
    "    for step in range(len(sentences) // batch_size):\n",
    "        idx = step * batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx + batch_size],\n",
    "                                                                               bert_tokenizer, max_length)\n",
    "        outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "        \n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        if step == 0:\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "        else:\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot, last_hidden_states))\n",
    "            \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f\"BERT processing time:{time2}s\\n\")\n",
    "    \n",
    "    return features_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_using_USE(sentences, b_size):\n",
    "    \"\"\"Transform the text of the question's body and title into USE embeddings.\"\"\"\n",
    "    # We don't want to use the cleaned text field with USE, only title + \" \" + body\n",
    "    sentences = [f\"{sentence[1]} {sentence[0]}\" for sentence in sentences.iterrows()]\n",
    "    \n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "    \n",
    "    us_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    \n",
    "    for step in range(len(sentences) // batch_size):\n",
    "        idx = step * batch_size\n",
    "        feat = us_encoder(sentences[idx:idx + batch_size])\n",
    "        \n",
    "        if step == 0:\n",
    "            features = feat\n",
    "        else:\n",
    "            features = np.concatenate((features, feat))\n",
    "            \n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f\"USE processing time:{time2}s\\n\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(questions_without_tags, text_transformation_method):\n",
    "    \"\"\"Transform the question text/body and title into words embeddings.\"\"\"\n",
    "    if text_transformation_method == \"Doc2VEC\":\n",
    "        return transform_text_using_Doc2VEC(questions_without_tags[\"text\"])\n",
    "        \n",
    "    elif text_transformation_method == \"BERT\":\n",
    "        max_length = 64\n",
    "        batch_size = 10\n",
    "        model_type = 'bert-base-uncased'\n",
    "        model = TFAutoModel.from_pretrained(model_type)\n",
    "        \n",
    "        return transform_text_using_BERT(model, model_type, questions_without_tags, max_length, batch_size)\n",
    "        \n",
    "    elif text_transformation_method == \"USE\":\n",
    "        batch_size = 10\n",
    "        return transform_text_using_USE(questions_without_tags, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_using_Doc2VEC(questions_without_tags):\n",
    "    \"\"\"Transform the text of the question's body and title into Doc2VEC embeddings.\"\"\"\n",
    "    time1 = time.time()\n",
    "    tagged_text = [TaggedDocument(words=text, tags=[str(index)])\n",
    "                   for index, text in enumerate(questions_without_tags)]\n",
    "\n",
    "    # dm=0 for DBOW, dm=1 for PV-DM\n",
    "    model = Doc2Vec(vector_size=30, min_count=2, epochs=80, dm=0)\n",
    "    model.build_vocab(tagged_text)\n",
    "    \n",
    "    model.train(tagged_text, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    embedded_text = [model.infer_vector(text.split(\" \")) for text in questions_without_tags]\n",
    "    \n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f\"Doc2VEC processing time:{time2}s\\n\")\n",
    "    \n",
    "    return embedded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_plot(results):\n",
    "    \"\"\"Generate the plot showing the performance with each words embedding method.\"\"\"\n",
    "    performance_plot = results.plot(kind=\"bar\", x=\"words_embedding_method\", figsize=(15, 8), rot=0,\n",
    "                                    title=\"Models Performance Sorted by Hamming Loss\")\n",
    "    \n",
    "    performance_plot.legend([f\"Hamming Loss\", f\"Jaccard Score\"])\n",
    "    performance_plot.title.set_size(20)\n",
    "    performance_plot.set(xlabel=None)\n",
    "    \n",
    "    # performance_plot.get_figure().savefig(f\"{RESULTS_PATH}/performance_plot.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_supervised_modeling(questions):\n",
    "    \"\"\"Find the best model using a GridSearchCV hyperoptimization for each words embedding method.\"\"\"\n",
    "    questions_df = DataFrame(questions)\n",
    "    \n",
    "    tags = MultiLabelBinarizer().fit_transform(questions_df['tags'])\n",
    "    questions_df['tags'].to_json(f\"{MODELS_PATH}/tags.json\")\n",
    "    \n",
    "    questions_without_tags = questions_df.drop(columns=['tags'], axis=1)\n",
    "    \n",
    "    results_df = DataFrame(columns=[\"words_embedding_method\", \"hamming_loss\", \"jaccard_score\"])\n",
    "    models = {}\n",
    "    for words_embedding_method in [\n",
    "        \"Doc2VEC\",\n",
    "        \"BERT\",\n",
    "        \"USE\"\n",
    "    ]:\n",
    "        print(f\"Starting supervised learning with words embedding method:{words_embedding_method}.\\n\")\n",
    "        \n",
    "        transformed_text = transform_text(questions_without_tags, words_embedding_method)\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(transformed_text, tags, test_size=0.2, random_state=42)\n",
    "        print(f\"training set size:{len(x_train)}, test set size:{len(x_test)}\\n\")\n",
    "        \n",
    "        default_model = XGBClassifier(n_estimators=50)\n",
    "        default_hyperparameters = {'estimator__max_depth': range(2, 12),}\n",
    "        \n",
    "        grid_search_cv = GridSearchCV(MultiOutputClassifier(estimator=default_model), default_hyperparameters,\n",
    "                                      cv=2,\n",
    "                                      scoring=make_scorer(metrics.hamming_loss, greater_is_better=False),\n",
    "                                      n_jobs=-1,\n",
    "                                      verbose=3\n",
    "                                      )\n",
    "        grid_search_cv.fit(x_train, y_train)\n",
    "        \n",
    "        best_parameters = grid_search_cv.best_params_\n",
    "        print(f\"\\nBest mean squared score:{grid_search_cv.best_score_} with params:{best_parameters}\")\n",
    "        \n",
    "        best_model = grid_search_cv.best_estimator_\n",
    "        models[words_embedding_method] = best_model\n",
    "        \n",
    "        predictions_test_y = grid_search_cv.best_estimator_.predict(x_test)\n",
    "        \n",
    "        hamming_loss = metrics.hamming_loss(y_true=y_test, y_pred=predictions_test_y)\n",
    "        jaccard_score = metrics.jaccard_score(y_true=y_test, y_pred=predictions_test_y, average='samples')\n",
    "        print(f\"Hamming loss:{hamming_loss}, jaccard_score:{jaccard_score}\\n\")\n",
    "        \n",
    "        results_df.loc[len(results_df)] = [words_embedding_method, hamming_loss, jaccard_score]\n",
    "        \n",
    "        send_results_to_mlflow(default_hyperparameters, best_model, hamming_loss, jaccard_score,\n",
    "                               words_embedding_method, x_train)\n",
    "        \n",
    "    results_df.sort_values(f\"hamming_loss\", ascending=True, inplace=True)\n",
    "    create_results_plot(results_df)\n",
    "    \n",
    "    save_best_model(models, results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(models, results_df):\n",
    "    \"\"\"Save the best model based on the hamming loss.\"\"\"\n",
    "    best_words_embedding_method = results_df.head(1)['words_embedding_method'].values[0]\n",
    "    joblib.dump(models[best_words_embedding_method], f\"{MODELS_PATH}/best_supervised_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_results_to_mlflow(default_hyperparameters, best_model, hamming_loss, jaccard_score, words_embedding_method,\n",
    "                           x_train):\n",
    "    \"\"\"Send data to the MLFlow server.\"\"\"\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(default_hyperparameters)\n",
    "        \n",
    "        mlflow.log_metric(\"hamming_loss\", hamming_loss)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard_score)\n",
    "        \n",
    "        mlflow.set_tag(\"Words embedding method\", words_embedding_method)\n",
    "        \n",
    "        signature = infer_signature(x_train, best_model.predict(x_train))\n",
    "        \n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"supervised-models\",\n",
    "            signature=signature,\n",
    "            input_example=x_train,\n",
    "            registered_model_name=\"XGBoost\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting supervised learning script. Please make sure you have a local MLFlow server running.\\n\")\n",
    "remove_last_saved_model()\n",
    "\n",
    "json_questions = load_cached_questions()\n",
    "\n",
    "questions = [{\n",
    "    \"body\": question['body'],\n",
    "    \"tags\": question['tags'],\n",
    "    \"title\": question['title']\n",
    "} for question in json_questions]\n",
    "\n",
    "print(f\"{len(questions)} questions loaded from cache.\\n\")\n",
    "\n",
    "questions = list(map(extract_and_clean_text, questions))\n",
    "print(\"Texts extracted and cleaned.\\n\")\n",
    "\n",
    "perform_supervised_modeling(questions)\n",
    "\n",
    "print(\"\\nSupervised learning now finished.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
