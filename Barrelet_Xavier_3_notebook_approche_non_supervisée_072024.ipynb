{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COHERENCE METRIC USED FOR HYPERPARAMETER OPTIMIZATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERENCE_METRIC = \"u_mass\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_QUESTIONS_FILE = 'cached_questions.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xavier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/xavier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_questions():\n",
    "    \"\"\"Load questions from the cache file.\"\"\"\n",
    "    with open(CACHED_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_text(question: dict):\n",
    "    \"\"\"Create a new 'text' field for each question containing the cleaned, tokenized and lemmatized title + body.\"\"\"\n",
    "    title = question['title']\n",
    "    body = question['body']\n",
    "    text = f\"{title} {body}\"\n",
    "    \n",
    "    for filter in [gsp.strip_tags,\n",
    "                   gsp.strip_punctuation,\n",
    "                   gsp.strip_multiple_whitespaces,\n",
    "                   gsp.strip_numeric,\n",
    "                   gsp.remove_stopwords,\n",
    "                   gsp.strip_short,\n",
    "                   gsp.lower_to_unicode]:\n",
    "        text = filter(text)\n",
    "        \n",
    "    tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # words_stemmed = (stemmer.stem(w) for w in words_without_short_words)\n",
    "    words_lemmatized = [lemmatizer.lemmatize(w) for w in tokenized_text]\n",
    "    question['text'] = \" \".join(words_lemmatized)\n",
    "\n",
    "    # bigrams = nltk.bigrams(tokenized_text)\n",
    "    # question['bigrams'] = [' '.join(bigram) for bigram in bigrams]\n",
    "\n",
    "    # trigrams = nltk.trigrams(tokenized_text)\n",
    "    # question['trigrams'] = [' '.join(trigram) for trigram in trigrams]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values_of_lda_model(corpus, id2word, texts, num_topics, alpha, eta):\n",
    "    \"\"\"Train a model and compute its coherence value.\"\"\"\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           # passes=1,\n",
    "                                           passes=10,\n",
    "                                           alpha=alpha,\n",
    "                                           eta=eta)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence=COHERENCE_METRIC)\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(questions):\n",
    "    \"\"\"Find the best hyperparameters for the LDA model and train it, visualizes the LDA topics and saves the model.\"\"\"\n",
    "    print(\"Starting the search of the best hyperparameters of the LDA model.\\n\")\n",
    "    \n",
    "    texts = [question['text'].split(\" \") for question in questions]\n",
    "    id2word = corpora.Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    best_hyperparameters: dict = get_best_hyperparameters_of_lda_model(corpus, id2word, texts)\n",
    "    \n",
    "    print(f\"Best hyperparameters found:{best_hyperparameters}.\\n\")\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=best_hyperparameters['num_topics'],\n",
    "                                           passes=5,\n",
    "                                           alpha=best_hyperparameters['alpha'],\n",
    "                                           eta=best_hyperparameters['eta'])\n",
    "    \n",
    "    print(\"Visualizing the topics of the LDA model.\\n\")\n",
    "    visualize_lda_topics(corpus, id2word, lda_model, best_hyperparameters['num_topics'])\n",
    "\n",
    "    print(\"Saving the LDA model.\\n\")\n",
    "    save_model(best_hyperparameters, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(best_hyperparameters, lda_model):\n",
    "    \"\"\"Save the LDA model\"\"\"\n",
    "    os.makedirs('models/unsupervised', exist_ok=True)\n",
    "    lda_model.save(f\"models/unsupervised/lda_model_with_{best_hyperparameters['num_topics']}_topics.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyperparameters_of_lda_model(corpus, id2word, texts):\n",
    "    \"\"\"Returns the best hyperparameters for the LDA model based on the coherence metric.\"\"\"\n",
    "    topics_range = range(2, 12, 1)\n",
    "    \n",
    "    alphas = list(np.arange(0.01, 2, 0.3))\n",
    "    alphas.append('symmetric')\n",
    "    alphas.append('asymmetric')\n",
    "    \n",
    "    etas = list(np.arange(0.01, 1, 0.3))\n",
    "    etas.append('symmetric')\n",
    "    etas.append(None)\n",
    "    \n",
    "    model_results = []\n",
    "\n",
    "    # OVERRIDES for test\n",
    "    # topics_range = [2]\n",
    "    # alphas = [1.51]\n",
    "    # etas = [0.9]\n",
    "    \n",
    "    pbar = tqdm(total=(len(etas) * len(alphas) * len(topics_range)))\n",
    "    \n",
    "    for num_topics in topics_range:\n",
    "        for alpha in alphas:\n",
    "            for eta in etas:\n",
    "                cv = compute_coherence_values_of_lda_model(corpus, id2word, texts, num_topics, alpha, eta)\n",
    "                \n",
    "                model_results.append({\"num_topics\": num_topics, \"alpha\": alpha, \"eta\": eta, \"cv\": cv})\n",
    "                pbar.update(1)\n",
    "                \n",
    "    pbar.close()\n",
    "    \n",
    "    return max(model_results, key=lambda x: x['cv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lda_topics(corpus, id2word, lda_model, num_topics):\n",
    "    \"\"\"Visualize the topics of the LDA model.\"\"\"\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    \n",
    "    # Workaround as pyLDAvis.display() doesn't work, even after pyLDAvis.enable_notebook(local=True)\n",
    "    os.mkdir(\"temp\")\n",
    "    pyLDAvis.save_html(LDAvis_prepared, 'temp/lda_results.html')\n",
    "    display(HTML('temp/lda_results.html'))\n",
    "    shutil.rmtree('temp', ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting unsupervised learning script.\n",
      "\n",
      "49996 questions loaded from cache.\n",
      "\n",
      "Texts extracted and cleaned.\n",
      "\n",
      "Starting the search of the best hyperparameters of the LDA model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████▎          | 384/540 [4:47:01<1:40:44, 38.75s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Starting unsupervised learning script.\\n\")\n",
    "\n",
    "json_questions = load_cached_questions()\n",
    "\n",
    "questions = [{\n",
    "    \"body\": question['body'],\n",
    "    \"tags\": question['tags'],\n",
    "    \"title\": question['title']\n",
    "} for question in json_questions]\n",
    "print(f\"{len(questions)} questions loaded from cache.\\n\")\n",
    "\n",
    "questions = list(map(extract_and_clean_text, questions))\n",
    "print(\"Texts extracted and cleaned.\\n\")\n",
    "\n",
    "train_lda_model(questions)\n",
    "\n",
    "print(\"\\nUnsupervised learning script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
