{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as gsp\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from stackapi import StackAPI\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a key if you want to download 50k questions, otherwise it will be 2500 questions.\n",
    "os.environ['SITE_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STACKAPI CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"SITE_API_KEY\") == \"\":\n",
    "    SITE = StackAPI('stackoverflow')\n",
    "    # 25 = limit with default of 100 results per page and no api key\n",
    "    SITE.max_pages = 25\n",
    "else:\n",
    "    SITE = StackAPI('stackoverflow', key=os.getenv(\"SITE_API_KEY\"))\n",
    "    # To get 50k results\n",
    "    SITE.max_pages = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All other notebooks are using this cached questions, run this one at least once before the others.\n",
    "CACHED_QUESTIONS_FILE = 'cached_questions.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_questions():\n",
    "    \"\"\"Load questions from the cache file.\"\"\"\n",
    "    with open(CACHED_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_questions():\n",
    "    \"\"\"Download and cache the questions in a file.\"\"\"\n",
    "    # https://stackapi.readthedocs.io/en/latest/user/complex.html\n",
    "    questions = SITE.fetch('questions',\n",
    "                           fromdate=datetime(2010, 1, 1),\n",
    "                           todate=datetime(2024, 8, 11),\n",
    "                           min=50,\n",
    "                           sort='votes',\n",
    "                           filter='withbody',\n",
    "                           # tagged='python'\n",
    "                           )\n",
    "    extracted_questions = questions['items']\n",
    "    \n",
    "    trimmed_questions = [{\n",
    "        \"body\": question['body'],\n",
    "        \"creation_date\": question['creation_date'],\n",
    "        \"score\": question['score'],\n",
    "        \"tags\": question['tags'],\n",
    "        \"title\": question['title']\n",
    "    } for question in extracted_questions]\n",
    "    \n",
    "    with open(CACHED_QUESTIONS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(trimmed_questions, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_text(question: dict):\n",
    "    \"\"\"Create a new 'text' field for each question containing the cleaned, tokenized and lemmatized title + body.\"\"\"\n",
    "    title = question['title']\n",
    "    body = question['body']\n",
    "    text = f\"{title} {body}\"\n",
    "    for filter in [gsp.strip_tags,\n",
    "                   gsp.strip_punctuation,\n",
    "                   gsp.strip_multiple_whitespaces,\n",
    "                   gsp.strip_numeric,\n",
    "                   gsp.remove_stopwords,\n",
    "                   gsp.strip_short,\n",
    "                   gsp.lower_to_unicode]:\n",
    "        text = filter(text)\n",
    "    tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # words_stemmed = (stemmer.stem(w) for w in words_without_short_words)\n",
    "    words_lemmatized = [lemmatizer.lemmatize(w) for w in tokenized_text]\n",
    "    question['text'] = \" \".join(words_lemmatized)\n",
    "\n",
    "    # bigrams = nltk.bigrams(tokenized_text)\n",
    "    # question['bigrams'] = [' '.join(bigram) for bigram in bigrams]\n",
    "\n",
    "    # trigrams = nltk.trigrams(tokenized_text)\n",
    "    # question['trigrams'] = [' '.join(trigram) for trigram in trigrams]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_word_clouds(questions):\n",
    "    \"\"\"Visualize the word clouds of the tags and words of the bodies\"\"\"\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3,\n",
    "                          contour_color='steelblue')\n",
    "    \n",
    "    generate_words_wordcloud(questions, wordcloud)\n",
    "    generate_tags_wordcloud(questions, wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags_wordcloud(questions, wordcloud):\n",
    "    \"\"\"Visualize the word clouds of the tags\"\"\"\n",
    "    unique_tags = set([tag for question in questions for tag in question['tags']])\n",
    "    joined_tags = ','.join(unique_tags)\n",
    "    print(f\"{len(unique_tags)} unique tags were found in the dataset.\\n\")\n",
    "    \n",
    "    cloud = wordcloud.generate(joined_tags)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # cloud.to_file(f\"{RESULTS_PATH}/tags_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_wordcloud(questions, wordcloud):\n",
    "    \"\"\"Visualize the word clouds of the words of the bodies\"\"\"\n",
    "    texts = [question['text'] for question in questions]\n",
    "    joined_texts = ','.join(texts)\n",
    "    \n",
    "    cloud = wordcloud.generate(joined_texts)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # cloud.to_file(f\"{RESULTS_PATH}/words_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_most_used_tags(questions):\n",
    "    \"\"\"Display the 50 most used tags and their count\"\"\"\n",
    "    top_50_df = get_most_used_tags(questions, 50)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(14, 14)\n",
    "    \n",
    "    sns.barplot(x=\"count\", y=\"tag\", data=top_50_df, color='#f56900', ax=axs)\n",
    "    plt.title('Most used tags')\n",
    "    # fig.savefig(f\"{RESULTS_PATH}/most_used_tags.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_used_tags(questions, count):\n",
    "    \"\"\"Returns the {count} most used tags\"\"\"\n",
    "    tags = defaultdict(int)\n",
    "    \n",
    "    for question in questions:\n",
    "        for tag in question['tags']:\n",
    "            tags[tag] += 1\n",
    "            \n",
    "    df: DataFrame = pd.DataFrame(list(tags.items()), columns=['tag', 'count'])\n",
    "    df.sort_values(by='count', ascending=False, inplace=True)\n",
    "    \n",
    "    return df.head(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_number_of_words_per_tag(questions):\n",
    "    \"\"\"Display the number of unique and total words for the 50 most used tags\"\"\"\n",
    "    words_per_tag = defaultdict(list)\n",
    "    \n",
    "    for question in questions:\n",
    "        for tag in question['tags']:\n",
    "            words_per_tag[tag] += question['text'].split(\" \")\n",
    "            \n",
    "    tags = []\n",
    "    for tag in words_per_tag.keys():\n",
    "        tags.append({\n",
    "            \"tag\": tag,\n",
    "            \"words\": len(words_per_tag[tag]),\n",
    "            \"unique_words\": len(list(set(words_per_tag[tag])))\n",
    "        })\n",
    "        \n",
    "    df: DataFrame = pd.DataFrame(tags)\n",
    "    df.sort_values(by='words', ascending=False, inplace=True)\n",
    "    \n",
    "    top_50_df = df.head(50)\n",
    "    generate_plot_with_words_per_tag(top_50_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot_with_words_per_tag(top_50_df):\n",
    "    \"\"\"Generate the plot with the number of unique and total words per tag\"\"\"\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(14, 14)\n",
    "    \n",
    "    sns.barplot(x=\"words\", y=\"tag\", data=top_50_df, color='#fce0cc', ax=axs)\n",
    "    sns.barplot(x=\"unique_words\", y=\"tag\", data=top_50_df, color='#f56900', ax=axs)\n",
    "    \n",
    "    plt.title('Number of unique words of the top 50 tags')\n",
    "    total_bar = mpatches.Patch(color='#fce0cc', label='Total')\n",
    "    unique_words_bar = mpatches.Patch(color='#f56900', label='Unique words')\n",
    "    fig.legend(handles=[total_bar, unique_words_bar])\n",
    "    \n",
    "    # fig.savefig(f\"{RESULTS_PATH}/words_per_tag.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_length_of_body_and_title(questions):\n",
    "    \"\"\"Display the distribution of the length of the bodies and titles\"\"\"\n",
    "    df: DataFrame = pd.DataFrame(questions)\n",
    "    \n",
    "    df['body_length'] = df['body'].apply(lambda x: len(x))\n",
    "    df['title_length'] = df['title'].apply(lambda x: len(x))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(14, 7)\n",
    "    \n",
    "    sns.histplot(df['body_length'], kde=True, ax=axs[0], color='#f56900')\n",
    "    sns.histplot(df['title_length'], kde=True, ax=axs[1], color='#f56900')\n",
    "    \n",
    "    axs[0].set_title('Distribution of the body length')\n",
    "    axs[1].set_title('Distribution of the title length')\n",
    "    \n",
    "    # fig.savefig(f\"{RESULTS_PATH}/length_of_body_and_title.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    boxplot = sns.boxplot(df, x=\"body_length\")\n",
    "    # boxplot.get_figure().savefig(f\"{RESULTS_PATH}/length_of_body_boxplot.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    boxplot_without_outliers = sns.boxplot(df, x=\"body_length\", showfliers=False)\n",
    "    # boxplot_without_outliers.get_figure().savefig(f\"{RESULTS_PATH}/length_of_body_boxplot_without_outliers.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dimensionality_reductions(questions):\n",
    "    \"\"\"Display the t-SNE visualizations of the texts linked to the five most used tags using TFIDF and CV\"\"\"\n",
    "    five_most_used_tags = get_most_used_tags(questions, 5)['tag']\n",
    "    all_texts = [question['text'] for question in questions]\n",
    "    \n",
    "    tfidf_vectorizer_model = TfidfVectorizer(stop_words='english', max_features=400)\n",
    "    tfidf_vectorizer_model.fit(all_texts)\n",
    "    \n",
    "    count_vectorizer_model = CountVectorizer(stop_words='english', max_features=400)\n",
    "    count_vectorizer_model.fit(all_texts)\n",
    "    \n",
    "    texts_df: DataFrame = DataFrame(columns=['tag', 'tsne-x-tfidf', 'tsne-y-tfidf', 'tsne-x-cv', 'tsne-y-cv'])\n",
    "    \n",
    "    for tag in five_most_used_tags:\n",
    "        texts_for_tag = [question['text'] for question in questions if tag in question['tags']]\n",
    "        \n",
    "        tsne_results_tfidf = get_tsne_results_with_model(texts_for_tag, tfidf_vectorizer_model)\n",
    "        tsne_results_cv = get_tsne_results_with_model(texts_for_tag, count_vectorizer_model)\n",
    "        \n",
    "        for index in range(len(tsne_results_tfidf)):\n",
    "            texts_df.loc[len(texts_df)] = [tag,\n",
    "                                           tsne_results_tfidf[index][0], tsne_results_tfidf[index][1],\n",
    "                                           tsne_results_cv[index][0], tsne_results_cv[index][1]]\n",
    "            \n",
    "    create_tsne_visualization_for_model(texts_df, \"tfidf\")\n",
    "    create_tsne_visualization_for_model(texts_df, \"cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsne_visualization_for_model(texts_df: DataFrame, model_type: str) -> None:\n",
    "    \"\"\"Create the t-SNE visualization for the given model type\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ctf_plot = sns.scatterplot(texts_df,\n",
    "                               x=f\"tsne-x-{model_type}\", y=f\"tsne-y-{model_type}\",\n",
    "                               hue=\"tag\", palette=\"bright\")\n",
    "    \n",
    "    model_name = \"TfidfVectorizer\" if model_type == \"tfidf\" else \"CountVectorizer\"\n",
    "    ctf_plot.set_title(f'Scatter plot of the texts linked to the five most used tags with {model_name}')\n",
    "    ctf_plot.set_xlabel(f'F1')\n",
    "    ctf_plot.set_ylabel(f'F2')\n",
    "    ctf_plot.grid(True)\n",
    "    \n",
    "    # ctf_plot.get_figure().savefig(f\"{RESULTS_PATH}/tsne_{model_type}.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsne_results_with_model(texts_for_tag, model):\n",
    "    \"\"\"Get the t-SNE results for the given model\"\"\"\n",
    "    ctf_transformed_text = model.transform(texts_for_tag)\n",
    "    \n",
    "    tsne_ctf = TSNE(n_components=2, perplexity=30, max_iter=2000, init='random', learning_rate=200, random_state=42)\n",
    "    return tsne_ctf.fit_transform(ctf_transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting analysis script.\\n\")\n",
    "\n",
    "if not exists(CACHED_QUESTIONS_FILE):\n",
    "    print(f\"Cached questions are missing, downloading them in {CACHED_QUESTIONS_FILE}.\\n\")\n",
    "    cache_questions()\n",
    "\n",
    "json_questions = load_cached_questions()\n",
    "\n",
    "questions = [{\n",
    "    \"body\": question['body'],\n",
    "    \"tags\": question['tags'],\n",
    "    \"title\": question['title']\n",
    "} for question in json_questions]\n",
    "print(f\"{len(questions)} questions loaded from cache.\\n\")\n",
    "\n",
    "# Too many false positives due to included code or technical words.\n",
    "# non_english_questions = [question for question in questions if langdetect.detect(question['body']) != 'en']\n",
    "\n",
    "cleaned_questions = list(map(extract_and_clean_text, questions))\n",
    "print(f\"Texts extracted and cleaned.\\n\")\n",
    "\n",
    "display_length_of_body_and_title(cleaned_questions)\n",
    "print(\"Length of body and title displayed.\\n\")\n",
    "\n",
    "display_most_used_tags(cleaned_questions)\n",
    "print(\"Most used tags displayed.\\n\")\n",
    "\n",
    "display_number_of_words_per_tag(cleaned_questions)\n",
    "print(\"Number of words per tag displayed.\\n\")\n",
    "\n",
    "visualize_word_clouds(cleaned_questions)\n",
    "print(\"Word clouds displayed.\\n\")\n",
    "\n",
    "visualize_dimensionality_reductions(cleaned_questions)\n",
    "print(\"Dimensionality reductions displayed.\\n\")\n",
    "\n",
    "print(\"Analysis script finished.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
