{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 20:06:46.170005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-22 20:06:46.183366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-22 20:06:46.187048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-22 20:06:46.198491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-22 20:06:47.109553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/xavier/Desktop/formation/project_5/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/xavier/Desktop/formation/project_5/.venv/lib/python3.12/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gensim.parsing.preprocessing as gsp\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from mlflow.models import infer_signature\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from pandas import DataFrame\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import *\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISC CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "#15k crashes during hyperoptimization with doc2vec already. 35k crashes with no hyperoptimization and USE.\n",
    "NUMBER_OF_QUESTIONS_USED_IN_TRAINING = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_QUESTIONS_FILE = 'cached_questions.json'\n",
    "MODELS_PATH = 'inferring_api/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xavier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/xavier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having multiprocessing issues between BERT and the GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_questions():\n",
    "    \"\"\"Load questions from the cache file.\"\"\"\n",
    "    with open(CACHED_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_saved_model():\n",
    "    \"\"\"Removes the content of the saved model.\"\"\"\n",
    "    shutil.rmtree(MODELS_PATH, ignore_errors=True)\n",
    "    os.makedirs(MODELS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_text(question: dict):\n",
    "    \"\"\"Create a new 'text' field for each question containing the cleaned, tokenized and lemmatized title + body.\"\"\"\n",
    "    title = question['title']\n",
    "    body = question['body']\n",
    "    text = f\"{title} {body}\"\n",
    "    \n",
    "    for filter in [gsp.strip_tags,\n",
    "                   gsp.strip_punctuation,\n",
    "                   gsp.strip_multiple_whitespaces,\n",
    "                   gsp.strip_numeric,\n",
    "                   gsp.remove_stopwords,\n",
    "                   gsp.strip_short,\n",
    "                   gsp.lower_to_unicode]:\n",
    "        text = filter(text)\n",
    "        \n",
    "    tokenized_text = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # words_stemmed = (stemmer.stem(w) for w in words_without_short_words)\n",
    "    words_lemmatized = [lemmatizer.lemmatize(w) for w in tokenized_text]\n",
    "    question['text'] = \" \".join(words_lemmatized)\n",
    "\n",
    "    # bigrams = nltk.bigrams(tokenized_text)\n",
    "    # question['bigrams'] = [' '.join(bigram) for bigram in bigrams]\n",
    "\n",
    "    # trigrams = nltk.trigrams(tokenized_text)\n",
    "    # question['trigrams'] = [' '.join(trigram) for trigram in trigrams]\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_inp_fct(sentences, bert_tokenizer, max_length):\n",
    "    \"\"\"Returns BERT variables for its prediction.\"\"\"\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    bert_inp_tot = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens=True,\n",
    "                                              max_length=max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "        \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0],\n",
    "                             bert_inp['token_type_ids'][0],\n",
    "                             bert_inp['attention_mask'][0]))\n",
    "        \n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_using_BERT(model, model_type, sentences, max_length, b_size):\n",
    "    \"\"\"Transform the text of the question's body and title into BERT embeddings.\"\"\"\n",
    "    # We don't want to use the cleaned text field with BERT, only title + \" \" + body\n",
    "    sentences = [f\"{sentence[1]} {sentence[0]}\" for sentence in sentences.iterrows()]\n",
    "    \n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    \n",
    "    time1 = time.time()\n",
    "    for step in range(len(sentences) // batch_size):\n",
    "        idx = step * batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx + batch_size],\n",
    "                                                                               bert_tokenizer, max_length)\n",
    "        outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "        \n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        if step == 0:\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "        else:\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot, last_hidden_states))\n",
    "            \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f\"BERT processing time:{time2}s\\n\")\n",
    "    \n",
    "    return features_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_using_USE(sentences, b_size):\n",
    "    \"\"\"Transform the text of the question's body and title into USE embeddings.\"\"\"\n",
    "    # We don't want to use the cleaned text field with USE, only title + \" \" + body\n",
    "    sentences = [f\"{sentence[1]} {sentence[0]}\" for sentence in sentences.iterrows()]\n",
    "    \n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "    \n",
    "    us_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    \n",
    "    for step in range(len(sentences) // batch_size):\n",
    "        idx = step * batch_size\n",
    "        feat = us_encoder(sentences[idx:idx + batch_size])\n",
    "        \n",
    "        if step == 0:\n",
    "            features = feat\n",
    "        else:\n",
    "            features = np.concatenate((features, feat))\n",
    "            \n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f\"USE processing time:{time2}s\\n\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(questions_without_tags, text_transformation_method):\n",
    "    \"\"\"Transform the question text/body and title into words embeddings.\"\"\"\n",
    "    if text_transformation_method == \"Doc2VEC\":\n",
    "        return transform_text_using_Doc2VEC(questions_without_tags[\"text\"])\n",
    "        \n",
    "    elif text_transformation_method == \"BERT\":\n",
    "        max_length = 64\n",
    "        batch_size = 10\n",
    "        model_type = 'bert-base-uncased'\n",
    "        model = TFAutoModel.from_pretrained(model_type)\n",
    "        \n",
    "        return transform_text_using_BERT(model, model_type, questions_without_tags, max_length, batch_size)\n",
    "        \n",
    "    elif text_transformation_method == \"USE\":\n",
    "        batch_size = 10\n",
    "        return transform_text_using_USE(questions_without_tags, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_using_Doc2VEC(questions_without_tags):\n",
    "    \"\"\"Transform the text of the question's body and title into Doc2VEC embeddings.\"\"\"\n",
    "    time1 = time.time()\n",
    "    tagged_text = [TaggedDocument(words=text, tags=[str(index)])\n",
    "                   for index, text in enumerate(questions_without_tags)]\n",
    "\n",
    "    # dm=0 for DBOW, dm=1 for PV-DM\n",
    "    model = Doc2Vec(vector_size=30, min_count=2, epochs=80, dm=0)\n",
    "    model.build_vocab(tagged_text)\n",
    "    \n",
    "    model.train(tagged_text, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    embedded_text = [model.infer_vector(text.split(\" \")) for text in questions_without_tags]\n",
    "    \n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f\"Doc2VEC processing time:{time2}s\\n\")\n",
    "    \n",
    "    return embedded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_plot(results):\n",
    "    \"\"\"Generate the plot showing the performance with each words embedding method.\"\"\"\n",
    "    performance_plot = results.plot(kind=\"bar\", x=\"words_embedding_method\", figsize=(15, 8), rot=0,\n",
    "                                    title=\"Models Performance Sorted by Jaccard score\")\n",
    "    \n",
    "    performance_plot.legend([f\"Hamming Loss\", f\"Jaccard Score\"])\n",
    "    performance_plot.title.set_size(20)\n",
    "    performance_plot.set(xlabel=None)\n",
    "    \n",
    "    # performance_plot.get_figure().savefig(f\"{RESULTS_PATH}/performance_plot.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_supervised_modeling(questions):\n",
    "    \"\"\"Find the best model using a GridSearchCV hyperoptimization for each words embedding method.\"\"\"\n",
    "    questions_df = DataFrame(questions)\n",
    "    \n",
    "    tags = MultiLabelBinarizer().fit_transform(questions_df['tags'])\n",
    "    questions_df['tags'].to_json(f\"{MODELS_PATH}/tags.json\")\n",
    "    \n",
    "    questions_without_tags = questions_df.drop(columns=['tags'], axis=1)\n",
    "    \n",
    "    results_df = DataFrame(columns=[\"words_embedding_method\", \"hamming_loss\", \"jaccard_score\"])\n",
    "    models = {}\n",
    "    for words_embedding_method in [\n",
    "        # \"Doc2VEC\",\n",
    "        # \"BERT\",\n",
    "        \"USE\"\n",
    "    ]:\n",
    "        print(f\"{datetime.datetime.now()} - Starting supervised learning with words embedding method:{words_embedding_method}.\\n\")\n",
    "        \n",
    "        # Doc2VEC    \n",
    "        # Best mean squared score:0.00044166666666666665 with params:{'estimator__max_depth': 9, 'estimator__max_features': 5}\n",
    "        # Hamming loss:0.0007047663118926423, jaccard_score:0.0017083333333333332\n",
    "        \n",
    "        # BERT\n",
    "        # Best mean squared score:0.0009333333333333332 with params:{'estimator__max_depth': 9, 'estimator__max_features': 5}\n",
    "        # Hamming loss:0.0007045349375289218, jaccard_score:0.0012916666666666664 \n",
    "        \n",
    "        # USE\n",
    "        # Best mean squared score:0.06750624999999999 with params:{'estimator__max_depth': 9, 'estimator__max_features': 5}\n",
    "        # Hamming loss:0.0006598796853308653, jaccard_score:0.08907499999999999\n",
    "        \n",
    "        transformed_text = transform_text(questions_without_tags, words_embedding_method)\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(transformed_text, tags, test_size=0.2, random_state=42)\n",
    "        print(f\"training set size:{len(x_train)}, test set size:{len(x_test)}\\n\")\n",
    "        \n",
    "        default_model = RandomForestClassifier(n_estimators=100)\n",
    "        default_hyperparameters = {'estimator__max_depth': range(2, 10), 'estimator__max_features': range(2, 6)}\n",
    "        # best_hyperparameters_with_10k_questions = {'estimator__max_depth': [9], 'estimator__max_features': [5]}\n",
    "\n",
    "        # grid_search_cv = GridSearchCV(MultiOutputClassifier(estimator=default_model), \n",
    "                                      # default_hyperparameters,\n",
    "                                      # best_hyperparameters_with_10k_questions,\n",
    "                                      # cv=2,\n",
    "                                      # scoring=make_scorer(metrics.jaccard_score, average='samples'),\n",
    "                                      # n_jobs=-1,\n",
    "                                      # verbose=3\n",
    "                                      # )\n",
    "        # grid_search_cv.fit(x_train, y_train)\n",
    "\n",
    "        # Using the best parameters found during hyperoptimisation with 10k questions\n",
    "        default_model = RandomForestClassifier(n_estimators=100, max_depth=9, max_features=5)\n",
    "        default_model.fit(x_train, y_train)\n",
    "        \n",
    "        # best_parameters = grid_search_cv.best_params_\n",
    "        # print(f\"\\nBest mean squared score:{grid_search_cv.best_score_} with params:{best_parameters}\")\n",
    "        \n",
    "        # best_model = grid_search_cv.best_estimator_\n",
    "        best_model = default_model\n",
    "        models[words_embedding_method] = best_model\n",
    "        \n",
    "        predictions_test_y = best_model.predict(x_test)\n",
    "        \n",
    "        hamming_loss = metrics.hamming_loss(y_true=y_test, y_pred=predictions_test_y)\n",
    "        jaccard_score = metrics.jaccard_score(y_true=y_test, y_pred=predictions_test_y, average='samples')\n",
    "        print(f\"Hamming loss:{hamming_loss}, jaccard_score:{jaccard_score}\\n\")\n",
    "        \n",
    "        results_df.loc[len(results_df)] = [words_embedding_method, hamming_loss, jaccard_score]\n",
    "        \n",
    "        send_results_to_mlflow(default_hyperparameters, best_model, hamming_loss, jaccard_score,\n",
    "                               words_embedding_method, x_train)\n",
    "        \n",
    "    results_df.sort_values(f\"jaccard_score\", ascending=False, inplace=True)\n",
    "    create_results_plot(results_df)\n",
    "    \n",
    "    save_best_model(models, results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(models, results_df):\n",
    "    \"\"\"Save the best model based on the hamming loss.\"\"\"\n",
    "    best_words_embedding_method = results_df.head(1)['words_embedding_method'].values[0]\n",
    "    joblib.dump(models[best_words_embedding_method], f\"{MODELS_PATH}/best_supervised_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_results_to_mlflow(default_hyperparameters, best_model, hamming_loss, jaccard_score, words_embedding_method,\n",
    "                           x_train):\n",
    "    \"\"\"Send data to the MLFlow server.\"\"\"\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(default_hyperparameters)\n",
    "        \n",
    "        mlflow.log_metric(\"hamming_loss\", hamming_loss)\n",
    "        mlflow.log_metric(\"jaccard_score\", jaccard_score)\n",
    "        \n",
    "        mlflow.set_tag(\"Words embedding method\", words_embedding_method)\n",
    "        \n",
    "        signature = infer_signature(x_train, best_model.predict(x_train))\n",
    "        \n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"supervised-models\",\n",
    "            signature=signature,\n",
    "            input_example=x_train,\n",
    "            registered_model_name=\"RandomForestClassifier\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting supervised learning script. Please make sure you have a local MLFlow server running.\n",
      "\n",
      "30000 questions loaded from cache.\n",
      "\n",
      "Texts extracted and cleaned.\n",
      "\n",
      "2024-08-22 20:07:02.459607 - Starting supervised learning with words embedding method:USE.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724332027.406214  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.555556  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.558492  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.562490  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.565248  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.567901  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.686942  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.688525  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724332027.689936  513387 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-22 20:07:07.691653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6184 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE processing time:31.0s\n",
      "\n",
      "training set size:24000, test set size:6000\n",
      "\n",
      "Hamming loss:0.00037606837606837606, jaccard_score:0.04318611111111111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting supervised learning script. Please make sure you have a local MLFlow server running.\\n\")\n",
    "remove_last_saved_model()\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "mlflow.set_experiment(\"Supervised Learning Experiment\")\n",
    "\n",
    "json_questions = load_cached_questions()\n",
    "\n",
    "questions = [{\n",
    "    \"body\": question['body'],\n",
    "    \"tags\": question['tags'],\n",
    "    \"title\": question['title']\n",
    "} for question in json_questions[:NUMBER_OF_QUESTIONS_USED_IN_TRAINING]]\n",
    "\n",
    "print(f\"{len(questions)} questions loaded from cache.\\n\")\n",
    "\n",
    "questions = list(map(extract_and_clean_text, questions))\n",
    "print(\"Texts extracted and cleaned.\\n\")\n",
    "\n",
    "perform_supervised_modeling(questions)\n",
    "\n",
    "print(\"\\nSupervised learning now finished.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
